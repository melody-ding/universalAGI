"""
LONG Path - Enhanced planner/executor with early exit conditions
"""

import logging
import time
import asyncio
from typing import Dict, Any, Optional, List
from dataclasses import dataclass

from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage
from config import settings

from search.multi_document_search import ContextBundle, build_grouped_context
from .smart_routing_config import SmartRoutingConfig
from .smart_probe import ProbeSignals
from .short_path import build_context_short_path

logger = logging.getLogger(__name__)


@dataclass
class SubQuery:
    """A sub-query generated by the planner"""
    query: str
    reasoning: str
    priority: int


@dataclass
class EvidenceBundle:
    """Accumulated evidence from subquery execution"""
    contexts: List[ContextBundle]
    total_docs: int
    total_segments: int
    avg_vec_sim: float
    fts_hit_rate: float
    execution_time: float


@dataclass
class LongPathResult:
    """Result from LONG path execution"""
    answer: str
    evidence: EvidenceBundle
    subqueries_executed: List[SubQuery]
    early_exit_reason: Optional[str] = None
    success: bool = True
    error: Optional[str] = None


def _should_early_exit(evidence: EvidenceBundle, config: SmartRoutingConfig, start_time: float) -> Optional[str]:
    """
    Check if early exit conditions are met
    
    Args:
        evidence: Current accumulated evidence
        config: Smart routing configuration
        start_time: Execution start time
        
    Returns:
        Early exit reason string if conditions met, None otherwise
    """
    elapsed_time = time.time() - start_time
    
    # Time budget check
    if elapsed_time >= config.long_budget_time_sec:
        return f"Time budget exceeded ({elapsed_time:.1f}s >= {config.long_budget_time_sec}s)"
    
    # Token budget check (approximate)
    total_context_length = sum(len(ctx.context_text) for ctx in evidence.contexts)
    estimated_tokens = total_context_length // 4  # rough token estimate
    if estimated_tokens >= config.long_budget_tokens:
        return f"Token budget exceeded (~{estimated_tokens} >= {config.long_budget_tokens})"
    
    # Evidence quality checks
    if (evidence.total_segments >= config.escalation.min_strong_segments and
        evidence.total_docs <= config.escalation.max_distinct_docs and
        (evidence.avg_vec_sim >= config.escalation.min_avg_vec_sim or 
         evidence.fts_hit_rate >= config.escalation.min_fts_hit_rate)):
        return f"Strong evidence found: {evidence.total_segments} segments from {evidence.total_docs} docs"
    
    return None


async def generate_subqueries(query: str, signals: ProbeSignals, config: SmartRoutingConfig) -> List[SubQuery]:
    """
    Generate focused subqueries with budget awareness
    
    Args:
        query: Original user query
        signals: Probe signals for context
        config: Smart routing configuration
        
    Returns:
        List of prioritized subqueries
    """
    llm = ChatOpenAI(
        model=settings.MODEL_NAME,
        temperature=0.1,  # Low temperature for consistent planning
        openai_api_key=settings.OPENAI_API_KEY
    )
    
    system_prompt = f"""You are a strategic query planner for document search. Generate 1-{config.long_max_subqueries} focused subqueries to thoroughly answer the user's question.

BUDGET CONSTRAINTS:
- Maximum {config.long_max_subqueries} subqueries
- Target: comprehensive coverage with minimal redundancy
- Each subquery should explore a distinct aspect

SUBQUERY GUIDELINES:
- Be specific and searchable
- Focus on factual information retrieval
- Avoid vague or overly broad queries
- Consider different angles/perspectives

Output JSON format:
{{
    "subqueries": [
        {{
            "query": "specific searchable query",
            "reasoning": "why this subquery is needed",
            "priority": 1
        }}
    ]
}}"""

    # Build context from probe signals
    probe_context = f"""
Original query: "{query}"

Probe analysis:
- Vector similarity: {signals.avg_vec_sim:.2f}
- FTS hit rate: {signals.fts_hit_rate:.2f}
- Document distribution: {signals.unique_docs} unique docs
- Query patterns: quotes/IDs={signals.has_quotes_or_ids}, temporal/compare={signals.has_compare_temporal_conditions}

Generate subqueries to comprehensively address this question within budget constraints."""

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=probe_context)
    ]

    try:
        response = llm.invoke(messages)
        
        # Parse JSON response
        import json
        import re
        
        json_match = re.search(r'\{.*\}', response.content, re.DOTALL)
        if json_match:
            data = json.loads(json_match.group())
            subqueries = []
            
            for sq_data in data.get("subqueries", []):
                subqueries.append(SubQuery(
                    query=sq_data.get("query", ""),
                    reasoning=sq_data.get("reasoning", ""),
                    priority=sq_data.get("priority", 1)
                ))
            
            # Sort by priority and limit
            subqueries.sort(key=lambda x: x.priority)
            return subqueries[:config.long_max_subqueries]
        
    except Exception as e:
        logger.warning(f"Subquery generation failed: {e}")
    
    # Fallback: use original query
    return [SubQuery(query=query, reasoning="Fallback to original query", priority=1)]


async def execute_subquery(subquery: SubQuery, config: SmartRoutingConfig) -> ContextBundle:
    """
    Execute a single subquery with optimized search
    
    Args:
        subquery: SubQuery to execute
        config: Smart routing configuration
        
    Returns:
        ContextBundle with retrieved context
    """
    logger.info(f"Executing subquery: {subquery.query[:100]}...")
    
    # Use the existing optimized search from SHORT path but with LONG path limits
    context = await build_context_short_path(subquery.query, config)
    
    logger.info(f"Subquery retrieved {len(context.blocks)} docs with {sum(len(b.snippets) for b in context.blocks)} segments")
    
    return context


def _deduplicate_and_merge_contexts(contexts: List[ContextBundle]) -> ContextBundle:
    """
    Deduplicate and merge multiple context bundles using MMR-style selection
    
    Args:
        contexts: List of context bundles to merge
        
    Returns:
        Merged ContextBundle with deduplicated content
    """
    if not contexts:
        return ContextBundle(query="", context_text="", blocks=[])
    
    # Collect all unique segments by ID to avoid exact duplicates
    seen_segments = set()
    merged_blocks = []
    all_queries = []
    
    for context in contexts:
        all_queries.append(context.query)
        
        for block in context.blocks:
            # Create a new block for this document if not already present
            existing_block = None
            for merged_block in merged_blocks:
                if merged_block.document_id == block.document_id:
                    existing_block = merged_block
                    break
            
            if existing_block is None:
                # Create new block
                from search.multi_document_search import ContextBlock
                existing_block = ContextBlock(
                    document_id=block.document_id,
                    title=block.title,
                    snippets=[]
                )
                merged_blocks.append(existing_block)
            
            # Add unique snippets
            for snippet in block.snippets:
                # Simple deduplication by content
                snippet_key = snippet.strip()
                if snippet_key not in seen_segments and len(snippet_key) > 10:
                    seen_segments.add(snippet_key)
                    existing_block.snippets.append(snippet)
    
    # Limit snippets per document to avoid overwhelming context
    for block in merged_blocks:
        block.snippets = block.snippets[:5]  # Max 5 snippets per doc in LONG path
    
    # Format merged context
    context_parts = []
    for block in merged_blocks:
        if block.snippets:  # Only include blocks with content
            context_parts.append(f"{{{block.title}}} [Document ID: {block.document_id}]")
            for snippet in block.snippets:
                context_parts.append(snippet)
            context_parts.append("")
    
    merged_context_text = "\n".join(context_parts).strip()
    
    return ContextBundle(
        query=" + ".join(all_queries),
        context_text=merged_context_text,
        blocks=[block for block in merged_blocks if block.snippets]
    )


async def synthesize_comprehensive_answer(query: str, evidence: EvidenceBundle, config: SmartRoutingConfig = None) -> str:
    """
    Synthesize comprehensive answer from accumulated evidence
    
    Args:
        query: Original user query
        evidence: Accumulated evidence bundle
        config: Smart routing configuration for token limits
        
    Returns:
        Comprehensive synthesized answer
    """
    from .token_manager import truncate_contexts_list, add_response_token_limit, validate_response_length
    
    config = config or DEFAULT_CONFIG
    
    # Truncate contexts if needed
    evidence.contexts = truncate_contexts_list(evidence.contexts, config)
    
    llm = ChatOpenAI(
        model=settings.MODEL_NAME,
        temperature=0.2,  # Slightly higher temperature for more natural synthesis
        openai_api_key=settings.OPENAI_API_KEY,
        max_tokens=config.max_response_tokens  # Set response token limit
    )
    
    # Merge all contexts
    merged_context = _deduplicate_and_merge_contexts(evidence.contexts)
    
    system_prompt = """You are an expert research analyst synthesizing comprehensive answers from multiple document sources.

SYNTHESIS GUIDELINES:
- Integrate information from multiple sources coherently
- Highlight agreements and note any discrepancies between sources
- Use mandatory citation tokens: [[doc:X, seg:Y]] where X is document ID and Y is segment ordinal
- The context shows documents in format: {Document Title} [Document ID: X]
- Each snippet shows [§ordinal] followed by text
- Use the EXACT Document ID shown in brackets and the EXACT segment ordinal from [§ordinal]
- Example: If you see "{Document ABC} [Document ID: 456]" and "[§7] Some text", cite as [[doc:456, seg:7]]
- Organize complex information clearly with logical flow
- Address multiple aspects of the question when relevant
- Acknowledge limitations if certain aspects lack coverage
- Always include citation tokens for verifiable facts

Provide a thorough, well-structured answer that demonstrates deep understanding of the retrieved information."""

    # Add token limit instructions
    system_prompt = add_response_token_limit(system_prompt, config)

    # Build comprehensive context summary with length check
    context_summary = f"""
Retrieved evidence from {evidence.total_docs} documents ({evidence.total_segments} segments):
Quality metrics: avg_vec_sim={evidence.avg_vec_sim:.2f}, fts_hit_rate={evidence.fts_hit_rate:.2f}

{merged_context.context_text}
"""

    # Ensure context summary fits within limits
    from .token_manager import estimate_tokens
    estimated_context_tokens = estimate_tokens(context_summary)
    if estimated_context_tokens > config.max_context_tokens:
        logger.warning(f"Context too long for synthesis: {estimated_context_tokens} > {config.max_context_tokens} tokens")
        # Truncate merged context further
        max_context_chars = config.max_context_tokens * 4 - 500  # Leave room for other text
        if len(merged_context.context_text) > max_context_chars:
            truncated_context = merged_context.context_text[:max_context_chars] + "\n\n[Context truncated due to length...]"
            context_summary = f"""
Retrieved evidence from {evidence.total_docs} documents ({evidence.total_segments} segments):
Quality metrics: avg_vec_sim={evidence.avg_vec_sim:.2f}, fts_hit_rate={evidence.fts_hit_rate:.2f}

{truncated_context}
"""

    user_prompt = f"""Original Question: {query}

{context_summary}

Provide a comprehensive, well-cited answer that synthesizes all relevant information to thoroughly address the question."""
    
    # Debug: Log the context being sent to LLM
    logger.info(f"LONG PATH CONTEXT FOR LLM:\n{merged_context.context_text[:500]}...")
    logger.info(f"Number of contexts in evidence: {len(evidence.contexts)}")
    for i, context in enumerate(evidence.contexts):
        logger.info(f"Context {i}: {len(context.blocks)} blocks")
        for j, block in enumerate(context.blocks):
            logger.info(f"  Block {j}: Doc ID {block.document_id}, Title: {block.title}")

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ]

    try:
        response = llm.invoke(messages)
        # Validate and truncate response if needed
        validated_response = validate_response_length(response.content, config)
        return validated_response
    except Exception as e:
        logger.error(f"LONG path synthesis failed: {e}")
        return f"I apologize, but I encountered an error while synthesizing the comprehensive answer: {str(e)}"


async def run_long_path(query: str, signals: ProbeSignals, config: SmartRoutingConfig) -> LongPathResult:
    """
    Execute the complete LONG path with early exit conditions
    
    Args:
        query: Original user query
        signals: Probe signals from routing
        config: Smart routing configuration
        
    Returns:
        LongPathResult with comprehensive answer
    """
    start_time = time.time()
    
    try:
        logger.info(f"Executing LONG path for: {query[:100]}...")
        
        # Step 1: Generate subqueries
        subqueries = await generate_subqueries(query, signals, config)
        logger.info(f"Generated {len(subqueries)} subqueries")
        
        # Step 2: Execute subqueries with early exit checks
        contexts = []
        executed_subqueries = []
        
        for i, subquery in enumerate(subqueries):
            # Check early exit before each subquery
            if i > 0:  # Skip check for first subquery
                evidence = EvidenceBundle(
                    contexts=contexts,
                    total_docs=len(set(block.document_id for ctx in contexts for block in ctx.blocks)),
                    total_segments=sum(sum(len(block.snippets) for block in ctx.blocks) for ctx in contexts),
                    avg_vec_sim=signals.avg_vec_sim,  # Use probe signals for approximation
                    fts_hit_rate=signals.fts_hit_rate,
                    execution_time=time.time() - start_time
                )
                
                early_exit_reason = _should_early_exit(evidence, config, start_time)
                if early_exit_reason:
                    logger.info(f"Early exit triggered: {early_exit_reason}")
                    break
            
            # Execute subquery
            context = await execute_subquery(subquery, config)
            contexts.append(context)
            executed_subqueries.append(subquery)
            
            logger.info(f"Subquery {i+1}/{len(subqueries)} completed")
        
        # Step 3: Build final evidence bundle
        final_evidence = EvidenceBundle(
            contexts=contexts,
            total_docs=len(set(block.document_id for ctx in contexts for block in ctx.blocks)),
            total_segments=sum(sum(len(block.snippets) for block in ctx.blocks) for ctx in contexts),
            avg_vec_sim=signals.avg_vec_sim,
            fts_hit_rate=signals.fts_hit_rate,
            execution_time=time.time() - start_time
        )
        
        # Step 4: Synthesize comprehensive answer
        answer = await synthesize_comprehensive_answer(query, final_evidence, config)
        
        execution_time = time.time() - start_time
        logger.info(f"LONG path completed in {execution_time:.1f}s: {final_evidence.total_docs} docs, {final_evidence.total_segments} segments")
        
        return LongPathResult(
            answer=answer,
            evidence=final_evidence,
            subqueries_executed=executed_subqueries,
            early_exit_reason=early_exit_reason if 'early_exit_reason' in locals() else None,
            success=True
        )
        
    except Exception as e:
        logger.error(f"LONG path failed: {e}")
        return LongPathResult(
            answer=f"I encountered an error during detailed analysis: {str(e)}",
            evidence=EvidenceBundle(contexts=[], total_docs=0, total_segments=0, 
                                    avg_vec_sim=0.0, fts_hit_rate=0.0, execution_time=0.0),
            subqueries_executed=[],
            success=False,
            error=str(e)
        )